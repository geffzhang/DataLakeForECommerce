# 2. Gathering data lake requirements

Normally the requirements are provided by the product manager of the project. He is responsible for collecting user 
story from all potential users(e.g. CEO that uses data lake to make decision. Departments that produce business data or
user behaviour data)

Note that we need to address not only the **functional requirements** (e.g. reporting, data viz), but also **non-functional
requirements** (e.g. scalability, availability, security, data governance, etc.)

Based on the requirements, 

## 2.1 An example of our data lake requirements

1. Collect and ingest business data, user behaviour data
2. Build multi-dimensional data model 
3. Analyze the data via different dimension/topic (e.g. products, locations, membership, etc) and produce report that
contains at least 100 indicators. For example, daily new/active membership subscriber, best sell products.
   
4. Provide an inter-active query system to allow exploratory data analysis.
5. Monitor data lake, be able to send alert(e.g. mail, sms) in case of error or exceptions.
6. Metadata management (Part of the data governance).
7. Data quality control
8. Key indicator monitoring. (e.g. When some indicators such as daily sales income change more than 30%, need to send alerts)
9. Security, such as authenticity, confidentiality(access control).
10. Scalability. For example, the data volume increase 100 TB each year, the data lake must be scalable to support this
11. Availability. For example, if some sever crash, can you allow some downtime?
12. Budge

## 2.2 Choose infrastructure  

Note requirement 10, 11 and 12 will influence how you choose your infrastructure(e.g. public cloud, premises server), 
and tools to implement the data lake. 

We need to estimate the data volume/velocity that you will deal with, based on this estimation we will decide how many 
servers/vms which we need to buy.

## 2.3 Choose frameworks

Features that you need to consider when choosing a framework:
- Data volume/velocity (For example, 10 MB daily mysql is enough. 100GB daily mysql cannot handle it.)
- Business logic requirements
- Maturity and documentation of the framework
- Cost of the development/deployment
- Cost of the maintenance/upgrade

Suppose we have done the evaluation, and our data lake will use the following frameworks
- Data ingestion: Flume, Sqoop, Kafka, Logstash, DataX
- Data storage: MySql, HDFS, HBase, Redis, MongoDB
- Data transformation/analysis: Hive, Spark. (hive on spark)
- Data query: Presto, kylin
- Data viz: Echarts, Superset
- Workflow management: Airflow, Azkaban
- Platform monitoring: Prometheus, Zabbix
- Metadata management: Atlas
- Security/Access control: Kerberos, Ranger