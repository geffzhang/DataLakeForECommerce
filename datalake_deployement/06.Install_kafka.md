# 6. Install a kafka cluster

## 6.0 Prerequisites

### 6.0.1 Install JDK
Kafka needs JDK or JRE to run. So the first step is to install jdk.

### 6.0.2 Install zookeeper
A critical dependency of Apache Kafka is Apache Zookeeper, which is a distributed configuration and 
synchronization service. Kafka uses ZooKeeper for:

- leadership election of Kafka Broker to elect Controller: The first arrived broker creates /kafka/controller 
  znode in zk. The other broker will fail because it already exists.
- partition replication leader/follower election
- discover new coming brokers
- stores basic metadata in Zookeeper such as information about topics(topic-partition pairs), brokers, consumer offsets (queue readers) and so on.
ETc.
  
Since all the critical information is stored in the Zookeeper, and it normally replicates this data across zk cluster, 
the failure of Kafka broker / Zookeeper does not affect the state of the Kafka cluster. Kafka will restore the state, 
once the Zookeeper restarts. These gives zero downtime for Kafka. The leader election between the Kafka broker is also 
done by using Zookeeper in the event of leader failure.
## 6.1 Creating a User for Kafka
```shell
# create a system user Kafka with a home dir
useradd -r kafka -m

# add Kafka to sudoer group
usermod -aG wheel kafka

# login as kafka
su -l kafka
```

## 6.2 Downloading and Extracting the Kafka Binaries
Download the kafka binary from http://kafka.apache.org/downloads.html

Note that Kafka uses Scala, and it's binary is compiled based on certain version of Scala. 
For example, kafka_2.11-1.0.0 means this Kafka version 1.0.0 is compiled based on scala 2.11; 
kafka_2.12-2.5.0 means this kafka of version 2.5.0 is compiled based on scala 2.12. 
So before you chose your kafka binary version, you need to check the scala version which runs on your server.

The latest stable verion is 2.8.0, and suppose our server runs scala 2.12. So we need to download the kafka_2.12-2.8.0.tgz

```shell

mkdir -p /opt/module/kafka

cd /opt/module/kafka

# Then download the binary 
wget https://miroir.univ-lorraine.fr/apache/kafka/2.8.0/kafka_2.12-2.8.0.tgz

# extract the binary
tar -xzvf kafka_2.12-2.8.0.tgz

# change the name
mv kafka_2.12-2.8.0 kafka-2.8.0

# check the result
[root@localhost kafka-2.8.0]# pwd
/opt/module/kafka-2.8.0
```
## 6.3 Setup env var
```shell
cd /etc/profile.d/

vim kafka.sh
# add following line
export KAFKA_HOME=/opt/module/kafka-2.8.0
export PATH=$PATH:$KAFKA_HOME/bin

# apply the config
source /etc/profile.d/kafka.sh
```
## 6.4 Configuring the Kafka Server
The main config file of a Kafka broker is on the **config/server.properties** file. The default config is 
enough to run a standalone Kafka broker as a test server. But we still want to highlight some key configuration attributes.

-【broker.id】 : Each broker has a unique id which is between 0~255. A good guideline is to set this value to something 
intrinsic to the host. For example, if your hostnames contain a unique number (such as host1.example.com , host2.example.com , etc.), 
that is a good choice for the broker.id value.

【log.dirs】 : Very important, becaue kafka write all transaction data under this folder，If you have multiple hard drive, 
you can set the multiple log dir, so data are split to multiple hard drive. This increase the throughput of the cluster.
If more than one path is specified(e.g. log.dirs=path1,path2), the broker will store partitions on them in 
a “least-used” fashion with one partition’s log segments stored within the same path. Note that the 
broker will place a new partition in the path that has the least number of partitions currently 
stored in it, not the least amount of disk space used in the following situations

【zookeeper.connect】 : This specifies which zk cluster to be used by kafka. The default configuration is localhost:2181.

【Listeners】 : This specifies which port the broker will listen to for the client request,The default port is 9092. 
It will get the value returned from ** java.net.InetAddress.getCanonicalHostName() if not configured**. 
The port number can be set to any available port by changing the port configuration parameter. Keep in mind that if a 
port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not a recommended configuration.

Below is an example of kafka configuration
```shell
# open the config file
cd /opt/module/kafka-2.8.0/config
vim server.properties

# Edit the server.properties file accordingly
# In dl1 the id is 1, in dl2 the id is 2, etc..
broker.id=1 (2,3 in other two machines)

#in a cluster environment with multiple zookeeper instances
zookeeper.connect=dl01.pengfei.org:2181,dl02.pengfei.org:2181,dl03.pengfei.org:2181

# set log dir path
log.dirs=fs_on_disk1/data/kafka-logs,fs_on_disk2/data/kafka-logs

# allow user to delete topic, disabled by default. 
delete.topic.enable=true

```

## 6.5 Start and stop the kafka cluster

```shell
# To start the server
# kafka-server-start.sh is the launch script. 
# - daemon option make kafka run as a daemon process
# we need to provide the path of the server.properties
sh /opt/module/kafka-2.8.0/bin/kafka-server-start.sh -daemon /opt/module/kafka-2.8.0/config/server.properties


# To stop the kafka process
sh /opt/module/kafka-2.8.0/bin/kafka-server-start.sh
```

Note, we need to run the above command on each node of the kafka cluster to start and stop the process. To make our 
life easier, we need a script to do it. Check the kafka_cluster_management.sh


## 6.6 Test the kafka cluster

### 6.6.1 Create a topic with one partition and three replication
```shell

#create a topic
sh kafka-topics.sh --create --zookeeper dl1.pengfei.org --replication-factor 3 --partitions 1 --topic Hello-Kafka

# List all topic in one broker
sh kafka-topics.sh --list --zookeeper dl1.pengfei.org:2181

#check status of a topic
sh kafka-topics.sh --describe --zookeeper dl1.pengfei.org --topic Hello-Kafka


```

### 6.6.2 Create a producer
Now we have created the topic, we can use **producer** to send message to the topic, and **consumer** to consume the
message.


```shell
# Start producer to send messages
# In the broker-list, you could put one or more kafka brokers
# In this example, we use only one, the name of the broker is defined in server.properties  
sh kafka-console-producer.sh --broker-list hadoop-nn.bioaster.org:9092 --topic Hello-Kafka
# it returns a producer console, all the text you enter below will be published in topic Hello-Kafka
>hello
>my first message
>my second message
>my third message
```

### 6.6.3 Create a consumer

Below we create two consumers to receive messages
```shell

###in server dl2.pengfei.org
sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning

hello
my first message
my second message
my third message
my fouth message
my fifth message


###in server dl3.pengfei.org
sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning

hello
my third message
my first message
my second message
my fouth message
my fifth message

#You could notice, if your consumer connecter after many message has been published
#the order may not be correct. But the latest message will be correct
```

# 6.7 Determine the appropriate number of nodes in a kafka cluster
In general, we use below equation to determine the number of nodes:

```text
node_number=2*(highest_message_throughput/100 * replication_number)+1
```

- highest_message_throughput: means that the biggest data velocity (volume per seconde) that you need to treat in a day. 
  For example, at 20:00 we have the most visite of our site. We need to treat 100MB data per second.
  
- replication_number: each message may have replication in a different nodes. Replication can increase availability of the cluster. 
  But it also increases network IO and memory usage. The default replication number is 1. But in generale, we have 2 or 3
  replications.
  
For example, suppose our highest_message_throughput=100MB/s, replication_number = 2
we will need 2*(100/100*2)+1=5 kafka nodes in the cluster.

## 6.7 Kafka stress test

Before you put your kafka cluster into production, you need to test the highest throughput that your cluster can handle.
Kafka already provide the stress test script for both producer and consumer. You can find these two test script in /bin
- kafka-producer-perf-test.sh
- kafka-consumer-perf-test.sh

### 6.7.1 Create a test topic
To fully test your cluster, the number of the partition in the test topic must be the same as the kafka node number. 
The replication number must be the default replication number of your production environment.

In our case, we have 3 node, and 2 replication. So the test topic should have 3 partition and 2 replication.
```shell
sh kafka-topics.sh --create --zookeeper dl1.pengfei.org:2181,dl2.pengfei.org:2181,dl3.pengfei.org:2181
 --replication-factor 2 --partitions 3 --topic stress-test
```

### 6.7.2 Producer stress test and tuning
We can use below command to stress test how the cluster handles the producer.

```shell
kafka-producer-perf-test.sh --topic stress-test --record-size 100 --num-records 10000000 --throughput -1
--producer-props bootstrap.servers=dl01.pengfei.org:9092,dl02.pengfei.org:9092,dl03.pengfei.org:9092
```

- record-size: specifies the size of each message send to the kafka cluster. The unity is Byte.
- num-records: specifies the number of records
- throughput: specifies how many messages per second that you will send to the cluster. If you set the value to -1, 
  it means as quickly as possible.
  
After running the above command, the kafka producer test script will return a test summery of the kafka cluster. In our
case, 3 nodes can handle about 15MB/s throughput. 

**Does 15MB/s throughput corresponds what we are waiting for?**
We have 3 nodes, each node has about 12.5MB/s network IO bandwidth. 3*12.5=37.5MB/s, due to the hardware and software
latency, we can only have 80% of the max network IO bandwidth, so that's about 30MB/s. In other words, our kafka
cluster can handle 30MB record per seconde(throughput). As we have 2 replication for all the message send to the cluster,
it means the kafka throughput is 30MB/2 =15MB per second. So our kafka cluster behave normally.

#### Tuning the producer 
The most important factors that impact the kafka cluster throughput is:
- network IO bandwidth
- number of the replication of the topic
- batch.size: It means how many records the producer will put together before send it. The default value is 16kB. 

##### Choose the most appropriate batch size. 
The batch size can't be too small or too big.
- If it is too small compare to your record size, it will reduce the throughput, because it will send a record without 
  using batch, if you have 100 record, it will send 100 times. But with a batch which can group 10 record in a batch, it
  only sends 10 times. This can reduce the over head of sending and receiving the message. Below example, we set the
  producer batch.size to 500B. You can notice the throughput drop to 1.5MB/s(15MB/s for 16k batch)
  
```shell
kafka-producer-perf-test.sh --topic stress-test --record-size 100 --num-records 10000000 --throughput -1
--producer-props bootstrap.servers=dl01.pengfei.org:9092,dl02.pengfei.org:9092,dl03.pengfei.org:9092 --batch.size=500
```
- If it is too big, it will increase latency. For example, if we set a batch.size=128KB, and we need 180 seconds to complete
a batch, the first record in the batch will have 180 second latency.
  
### 6.7.3 Consumer stress test and tuning
We can use below command to stress test how the cluster handles the consumer.

```shell
kafka-consumer-perf-test.sh --topic stress-test --broker-list dl01.pengfei.org:9092,dl02.pengfei.org:9092,dl03.pengfei.org:9092
--fetch-size 10000 --messages 100000000 --threads 1
```

- broker-list: specifies the kafka nodes list, note it uses the same port(9092) as the producer
- fetch-size: specifies the size of the message of each fetch
- messages: specifies the total message number that the consumer will consume.
- threads: specifies how many thread will be used to consume the message. But if you look at the source code
it does not use this argument, so you can set it to 1 or 100, it changes nothing.
  
After running the above command, you will receive a summery as shown below. The most important is **MB.sec** which
indicates the message consumer per second. In our case it's 10.0647MB per second.

```text
start.time: 2020-10-26 09:21:39:216
end.time: 2020-10-26 09:22:00:973
data.consumed.in.MB: 2907.0630
**MB.sec: 10.0647**
data.consumed.in.nMsg: 3000270
nMsg.sec: 137899.0670
rebalance.time.ms: 1603704103558
fetch.time.ms: -1603704081801
fetch.MB.sec: -0.0000
fetch.nMsg.sec: -0.0019
```

Normally, the consumer could have 30MB/s throughput, but we only reached 10MB/s. So we need to tune the consumer.

#### Tuning the producer
The most important factors that impact the kafka consumer throughput is:
- network IO bandwidth
- fetch-size:

We can try to increase the fetch-size from 10000 to 100000, you can notice the MB.sec increase to 13

## 6.8 Determine the most appropriate partition number for your topic

In general, when you increase the partition number, you increase the parallelism of your topic. Because producers
can send messages to different partitions in parallel, across different brokers in parallel, and read by different 
consumers in parallel. But if your broker number is fixed, and your network IO is fixed, if the partition number is 
greater than the broker number, it means two or more partitions are in the same broker. In this case, when you increase 
the partition number, the throughput will not increase. Because the network IO of a broker is fixed.


Suppose we have enough broker(e.g. 1 partition per broker), the producer throughput is 100MB/s(tp), the consumer
throughput is 50MB/s(tc), and each broker throughput(tb) is 20MB/s,

We can use below equation to calculate the partition number
```text
partition_number=max(tp,tc)/tb
```

In our case, we will need max(100,50)/20=5 partition on 5 broker to handle 100MB.
