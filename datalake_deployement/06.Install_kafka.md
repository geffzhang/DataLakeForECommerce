# 6. Install a kafka cluster

## 6.0 Prerequisites

### 6.0.1 Install JDK
Kafka needs JDK or JRE to run. So the first step is to install jdk.

### 6.0.2 Install zookeeper
A critical dependency of Apache Kafka is Apache Zookeeper, which is a distributed configuration and 
synchronization service. Kafka uses ZooKeeper for:

- leadership election of Kafka Broker to elect Controller: The first arrived broker creates /kafka/controller 
  znode in zk. The other broker will fail because it already exists.
- partition replication leader/follower election
- discover new coming brokers
- stores basic metadata in Zookeeper such as information about topics(topic-partition pairs), brokers, consumer offsets (queue readers) and so on.
ETc.
  
Since all the critical information is stored in the Zookeeper, and it normally replicates this data across zk cluster, 
the failure of Kafka broker / Zookeeper does not affect the state of the Kafka cluster. Kafka will restore the state, 
once the Zookeeper restarts. These gives zero downtime for Kafka. The leader election between the Kafka broker is also 
done by using Zookeeper in the event of leader failure.
## 6.1 Creating a User for Kafka
```shell
# create a system user Kafka with a home dir
useradd -r kafka -m

# add Kafka to sudoer group
usermod -aG wheel kafka

# login as kafka
su -l kafka
```

## 6.2 Downloading and Extracting the Kafka Binaries
Download the kafka binary from http://kafka.apache.org/downloads.html

Note that Kafka uses Scala, and it's binary is compiled based on certain version of Scala. 
For example, kafka_2.11-1.0.0 means this Kafka version 1.0.0 is compiled based on scala 2.11; 
kafka_2.12-2.5.0 means this kafka of version 2.5.0 is compiled based on scala 2.12. 
So before you chose your kafka binary version, you need to check the scala version which runs on your server.

The latest stable verion is 2.8.0, and suppose our server runs scala 2.12. So we need to download the kafka_2.12-2.8.0.tgz

```shell

mkdir -p /opt/module/kafka

cd /opt/module/kafka

# Then download the binary 
wget https://miroir.univ-lorraine.fr/apache/kafka/2.8.0/kafka_2.12-2.8.0.tgz

# extract the binary
tar -xzvf kafka_2.12-2.8.0.tgz

# change the name
mv kafka_2.12-2.8.0 kafka-2.8.0

# check the result
[root@localhost kafka-2.8.0]# pwd
/opt/module/kafka-2.8.0
```
## 6.3 Setup env var
```shell
cd /etc/profile.d/

vim kafka.sh
# add following line
export KAFKA_HOME=/opt/module/kafka-2.8.0
export PATH=$PATH:$KAFKA_HOME/bin

# apply the config
source /etc/profile.d/kafka.sh
```
## 6.4 Configuring the Kafka Server
The main config file of a Kafka broker is on the **config/server.properties** file. The default config is 
enough to run a standalone Kafka broker as a test server. But we still want to highlight some key configuration attributes.

-【broker.id】 : Each broker has a unique id which is between 0~255. A good guideline is to set this value to something 
intrinsic to the host. For example, if your hostnames contain a unique number (such as host1.example.com , host2.example.com , etc.), 
that is a good choice for the broker.id value.

【log.dirs】 : Very important, becaue kafka write all transaction data under this folder，If you have multiple hard drive, 
you can set the multiple log dir, so data are split to multiple hard drive. This increase the throughput of the cluster.
If more than one path is specified(e.g. log.dirs=path1,path2), the broker will store partitions on them in 
a “least-used” fashion with one partition’s log segments stored within the same path. Note that the 
broker will place a new partition in the path that has the least number of partitions currently 
stored in it, not the least amount of disk space used in the following situations

【zookeeper.connect】 : This specifies which zk cluster to be used by kafka. The default configuration is localhost:2181.

【Listeners】 : This specifies which port the broker will listen to for the client request,The default port is 9092. 
It will get the value returned from ** java.net.InetAddress.getCanonicalHostName() if not configured**. 
The port number can be set to any available port by changing the port configuration parameter. Keep in mind that if a 
port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not a recommended configuration.

Below is an example of kafka configuration
```shell
# open the config file
cd /opt/module/kafka-2.8.0/config
vim server.properties

# Edit the server.properties file accordingly
# In dl1 the id is 1, in dl2 the id is 2, etc..
broker.id=1 (2,3 in other two machines)

#in a cluster environment with multiple zookeeper instances
zookeeper.connect=dl01.pengfei.org:2181,dl02.pengfei.org:2181,dl03.pengfei.org:2181

# set log dir path
log.dirs=fs_on_disk1/data/kafka-logs,fs_on_disk2/data/kafka-logs

# allow user to delete topic, disabled by default. 
delete.topic.enable=true

```

## 6.5 Start and stop the kafka cluster

```shell
# To start the server
# kafka-server-start.sh is the launch script. 
# - daemon option make kafka run as a daemon process
# we need to provide the path of the server.properties
sh /opt/module/kafka-2.8.0/bin/kafka-server-start.sh -daemon /opt/module/kafka-2.8.0/config/server.properties


# To stop the kafka process
sh /opt/module/kafka-2.8.0/bin/kafka-server-start.sh
```

Note, we need to run the above command on each node of the kafka cluster to start and stop the process. To make our 
life easier, we need a script to do it. Check the kafka_cluster_management.sh


## 6.6 Test the kafka cluster

### 6.6.1 Create a topic with one partition and three replication
```shell

#create a topic
sh kafka-topics.sh --create --zookeeper dl1.pengfei.org --replication-factor 3 --partitions 1 --topic Hello-Kafka

# List all topic in one broker
sh kafka-topics.sh --list --zookeeper dl1.pengfei.org:2181

#check status of a topic
sh kafka-topics.sh --describe --zookeeper dl1.pengfei.org --topic Hello-Kafka


```

### 6.6.2 Create a producer
Now we have created the topic, we can use **producer** to send message to the topic, and **consumer** to consume the
message.


```shell
# Start producer to send messages
# In the broker-list, you could put one or more kafka brokers
# In this example, we use only one, the name of the broker is defined in server.properties  
sh kafka-console-producer.sh --broker-list hadoop-nn.bioaster.org:9092 --topic Hello-Kafka
# it returns a producer console, all the text you enter below will be published in topic Hello-Kafka
>hello
>my first message
>my second message
>my third message
```

### 6.6.3 Create a consumer

Below we create two consumers to receive messages
```shell

###in server dl2.pengfei.org
sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning

hello
my first message
my second message
my third message
my fouth message
my fifth message


###in server dl3.pengfei.org
sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning

hello
my third message
my first message
my second message
my fouth message
my fifth message

#You could notice, if your consumer connecter after many message has been published
#the order may not be correct. But the latest message will be correct
```

# 6.7 Determine the appropriate number of nodes in a kafka cluster
In general, we use below equation to determine the number of nodes:

```text
node_number=2*(highest_message_throughput/100 * replication_number)+1
```

- highest_message_throughput: means that the biggest data velocity (volume per seconde) that you need to treat in a day. 
  For example, at 20:00 we have the most visite of our site. We need to treat 100MB data per second.
  
- replication_number: each message may have replication in a different nodes. Replication can increase availability of the cluster. 
  But it also increases network IO and memory usage. The default replication number is 1. But in generale, we have 2 or 3
  replications.
  
For example, suppose our highest_message_throughput=100MB/s, replication_number = 2
we will need 2*(100/100*2)+1=5 kafka nodes in the cluster.

