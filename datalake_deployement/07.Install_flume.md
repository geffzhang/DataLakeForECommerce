# 7. Flume

## 7.1 Install flume
You can download the latest stable version from this https://www.apache.org/dist/flume/stable/.
The current latest stable version is 1.9.0

Download the tar ball and untar it under **/opt/module/**

### 7.1.1 Configure env var for flume
Suppose the flume home path is /opt/module/flume-1.9.0

```shell

#add the flume home to the path
vim /etc/profile.d/flume.sh

# add following line to the file
export FLUME_HOME=/opt/module/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin

# check the flume home
source flume.sh 
echo $FLUME_HOME

```

### 7.1.2 Dependencies conflict with hadoop

On the same server, if you have installed hadoop 3.1.3 and flume 1.9.0. You will have a dependencies conflict. 
They both use the google core java lib Guava, but with different version. 

The easiest way is to delete the guava-11.0.2.jar under flume/lib/. Note, you must set env var for hadoop home. 
Otherwise, you will get class not found exception

## 7.2 Configure flume
All the important flume configuration files are located under **/path/to/flume/conf**. There are the four following 
files:

- flume-conf.properties: defines the flume agent
- flume-env.sh: defines the env var for running flume 
- flume-env.ps1.template
- log4j.properties

The most important config file for flume is **flume-env.sh**. Below is the minimum config which we need to run 
the flume.

```shell
vim flume-env.sh

#add the export java home
export JAVA_HOME=/opt/JAVA/jdk1.8.0_144

# Check the flume version
sh flume-ng version
```

## 7.3 Create a flume agent
A flume agent is an independent daemon process (JVM) in Flume. It receives the data (events) from clients or other 
agents and forwards it to its next destination (sink or agent). Flume may have more than one agent.

A flume agent contains three important components:
- source: receives data from the data generators and transfers it to one or more channels in the form of Flume events.
- channel: is a transient store which receives the events from the source and buffers them till they are consumed by 
  sinks. It acts as a bridge between the sources and the sinks.
- sink: stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels
  and delivers it to the destination. The destination of the sink might be another agent or the central stores.

### 7.3.1 Some popular sources
- Taildir: Watch the specified files, and tail them in nearly real-time once detected new lines appended to each file. 
  If the new lines are being written, this source will retry reading them in wait for the completion of the write.
  **This source is reliable and will not miss data** even when the tailing files rotate. It periodically writes the 
  last read position of each files on the given position file in JSON format. If Flume is stopped or down for some 
  reason, it can restart tailing from the position written on the existing position file.

- Avro source
- Thrift source 
  
- twitter 1% source 

### 7.3.2 Some popular channel
- File system channel: Reliable, high latency
- Memory channel: low latency, but reliable
- JDBC channel:
- kafka channel: low latency, reliable

## 7.4 A Flume agent example

Below is a Flume agent example, it uses a **TAILDIR** source to monitor a directory, if a file is updated or new
files are created, flume event will be created and sent to the linked channels. Here we use a **kafka channel**
to receive event generated by the **TAILDIR** channel. In the source, we added a custom interceptor to ignor
incomplete json file.

```text
## define flume agent component, a1 is the name of agent, it has one source s1, one channel c1
a1.sources=s1
a1.channels=c1

## define source s1
a1.sources.s1.type=TAILDIR
a1.sources.s1.filegroups=f1
a1.sources.s1.filegroups.f1=/tmp/logs/app_log.*
# to store upload offset, can resume upload after failure
a1.sources.s1.positionFile=/tmp/flume/taildir_position.json
# add an intercepter, can do simple data cleaning, note if you put complexe logic, it can decrease
# the throughput of Flume, here we just check if the json file is complete
a1.sources.s1.interceptors=i1
a1.sources.s1.interceptors.i1.type=

## define channel c1
# we use kafka channel
a1.channels.c1.type=org.apache.flume.channel.kafka,KafkaChannel
# we specify kafka broker url
a1.channels.c1.kafka.bootstrap.servers=dl1.pengfei.org:9092,dl2.pengfei.org:9092,dl3.pengfei.org:9092
# specify kafka topic name
a1.channels.c1.kafka.topic=test_topic
# remove flume event header when putting event message to kafka channel to avoid kafka consumer remove it manually
a1.channels.c1.parseAsFlumeEvent=false

## link the component
# link source s1 to channel c1
a1.sources.s1.channels=c1

```