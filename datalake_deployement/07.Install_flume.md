# 7. Flume

## 7.1 Install flume
You can download the latest stable version from this https://www.apache.org/dist/flume/stable/.
The current latest stable version is 1.9.0

Download the tar ball and untar it under **/opt/module/**

### 7.1.1 Configure env var for flume
Suppose the flume home path is /opt/module/flume-1.9.0

```shell

#add the flume home to the path
vim /etc/profile.d/flume.sh

# add following line to the file
export FLUME_HOME=/opt/module/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin

# check the flume home
source flume.sh 
echo $FLUME_HOME

```

### 7.1.2 Dependencies conflict with hadoop

On the same server, if you have installed hadoop 3.1.3 and flume 1.9.0. You will have a dependencies conflict. 
They both use the google core java lib Guava, but with different version. 

The easiest way is to delete the guava-11.0.2.jar under flume/lib/. Note, you must set env var for hadoop home. 
Otherwise, you will get class not found exception

## 7.2 Configure flume
All the important flume configuration files are located under **/path/to/flume/conf**. There are the four following 
files:

- flume-conf.properties: defines the flume agent
- flume-env.sh: defines the env var for running flume 
- flume-env.ps1.template
- log4j.properties

The most important config file for flume is **flume-env.sh**. Below is the minimum config which we need to run 
the flume.

```shell
vim flume-env.sh

#add the export java home
export JAVA_HOME=/opt/JAVA/jdk1.8.0_144

# Check the flume version
sh flume-ng version
```

## 7.3 Create a flume agent
A flume agent is an independent daemon process (JVM) in Flume. It receives the data (events) from clients or other 
agents and forwards it to its next destination (sink or agent). Flume may have more than one agent.

A flume agent contains three important components:
- source: receives data from the data generators and transfers it to one or more channels in the form of Flume events.
- channel: is a transient store which receives the events from the source and buffers them till they are consumed by 
  sinks. It acts as a bridge between the sources and the sinks.
- sink: stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels
  and delivers it to the destination. The destination of the sink might be another agent or the central stores.

### 7.3.1 Some popular sources
- Taildir: Watch the specified files, and tail them in nearly real-time once detected new lines appended to each file. 
  If the new lines are being written, this source will retry reading them in wait for the completion of the write.
  **This source is reliable and will not miss data** even when the tailing files rotate. It periodically writes the 
  last read position of each files on the given position file in JSON format. If Flume is stopped or down for some 
  reason, it can restart tailing from the position written on the existing position file.

- Avro source
- Thrift source 
  
- twitter 1% source 

### 7.3.2 Some popular channel
- File system channel: Reliable, high latency
- Memory channel: low latency, but reliable
- JDBC channel:
- kafka channel: low latency, reliable

## 7.4 A Flume agent example

Below is a Flume agent example, it uses a **TAILDIR** source to monitor a directory, if a file is updated or new
files are created, flume event will be created and sent to the linked channels. Here we use a **kafka channel**
to receive event generated by the **TAILDIR** channel. In the source, we added a **custom interceptor** to ignore
incomplete json file. 

 

```text
## define flume agent component, a1 is the name of agent, it has one source s1, one channel c1
a1.sources=s1
a1.channels=c1

## define source s1
a1.sources.s1.type=TAILDIR
a1.sources.s1.filegroups=f1
a1.sources.s1.filegroups.f1=/tmp/logs/app_log.*
# to store upload offset, can resume upload after failure
a1.sources.s1.positionFile=/tmp/flume/taildir_position.json
# add an intercepter, can do simple data cleaning, note if you put complexe logic, it can decrease
# the throughput of Flume, here we just check if the json file is complete
a1.sources.s1.interceptors=i1
# here we use a custom interceptor(https://github.com/pengfei99/CustomFlumeInterceptor.git)
# The $Build means call the static builder(inner class of RemoveIncompleteJson) class to generate an interceptor instance
a1.sources.s1.interceptors.i1.type=org.pengfei.flume.interceptors.RemoveIncompleteJson$Builder

## define channel c1
# we use kafka channel
a1.channels.c1.type=org.apache.flume.channel.kafka,KafkaChannel
# we specify kafka broker url
a1.channels.c1.kafka.bootstrap.servers=dl01.pengfei.org:9092,dl02.pengfei.org:9092,dl03.pengfei.org:9092
# specify kafka topic name
a1.channels.c1.kafka.topic=test_topic
# remove flume event header when putting event message to kafka channel to avoid kafka consumer remove it manually
a1.channels.c1.parseAsFlumeEvent=false

## link the component
# link source s1 to channel c1
a1.sources.s1.channels=c1

```

### 7.4.1 The custom interceptor 
The custom interceptor is written in java, and compile to a jar file with required dependencies. For more details on how
to write a flume custom interceptor, please go check https://github.com/pengfei99/CustomFlumeInterceptor

Once you have the jar file, you need to place it under /path/to/flume/lib/. So that the agent can find the interceptor

### 7.4.1 Run the flume agent

Suppose we put the above flume config file on dl01, dl02 server, with name file-flume-kafka.conf. To run it, we can use 
the following command

```shell
# on dl01
sh flume-ng agent --name a1 --conf-file /etc/flume/agent-conf/file-flume-kafka.conf &

# on dl02
sh flume-ng agent --name a1 --conf-file /etc/flume/agent-conf/file-flume-kafka.conf &

# you can check the flume agent process, by using jps, it's called application 
jps
```

## 7.5 Flume agent management

As you can notice in the previous section, the flume agent is launched one by one. And these processes are not daemon 
processes, which means when you close the terminal that launches it. The process will be killed.

So we need to use the **nohup** command, Nohup (stands for no hangup) is a command that ignores the HUP signal. 
The HUP signal is basically a signal that is delivered to a process when its associated shell is terminated. 
Usually, when we log out, then all the running programs and processes are hangup or stopped. If we want to continue 
running the process even after logout or disconnection from the current shell, we can use the nohup command.

If we use the **nohup** command to launch the flume agent, how do we stop it after?

## 7.6 Another Flume agent example

### 7.6.1 Agent Architecture
In section 7.4, we wrote a flume agent that monitors a directory and write the newly added lines to a kafka channel. 
In this section, we will write another flume agent which can consume the flume event in the kafka channel and write 
them to a hdfs sink. 

We will use a file channel to link the kafka source and hdfs sink. File channel stores flume events in hard drives, to
increase performance, it creates an index of these events, and the index are stored in memory. To ensure the 
availability, the index in memory will have too backup copies on disk.

### 7.6.2 time drift problem
Suppose we collect all the data of day 1, and these data will be treated on day2. All the data of the same day are 
stored in one folder. If an event is generated at 23:59 of day 1, due to the latency of the data pipeline, it arrives
at the hdfs sink at 00:03 of day2. So the timestamp of this event is day1, but it will be stored on the folder of day2.

To resolve this problem, we need to add a **time interceptor** that will classify event by using their time stamp, not 
the arrival time

### 7.6.3 Agent specification

```text
## define flume agent component, a1 is the name of agent, it has one source s1, one channel c1, one sink k1
a1.sources=s1
a1.channels=c1
a1.sinks=k1

## define source s1
a1.sources.s1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.s1.kafka.bootstrap.server=dl01.pengfei.org:9092,dl02.pengfei.org:9092,dl03.pengfei.org:9092
a1.sources.s1.kafka.topics=test_topic
# when we use kafka source, we need to define how events are organized in a batch to increase performance, the number
# is in byte. The kafka source will sent the batch if the total event size in the batch reaches 5000B. 
a1.source.s1.batchSize=5000
# To avoid high latency, we can add a max wait time. It means if the duration is met, the batch will be sent even though
# the batch is not full. In our example,if batch is not full, and batch already wait for 2000 milliseconds, batch will be sent 
a1.sources.s1.batchDurationMillis=2000
# add an intercepter
a1.sources.s1.interceptors=i1
# here we use a custom interceptor(https://github.com/pengfei99/CustomFlumeInterceptor.git)
# The $Build means call the static builder(inner class of RemoveIncompleteJson) class to generate an interceptor instance
a1.sources.s1.interceptors.i1.type=org.pengfei.flume.interceptors.TimeStampInterceptor$Builder

## define file channel c1
a1.channels.c1.type=org.apache.flume.channel.file
# we add multiple folder which are on different hard drive. It can increase the performance
# If not specified, flume uses ~/.flume to store the event
a1.channels.c1.dataDirs=/tmp/flume/channels/consume_agent1/
# Add a checkpoint to allow file channel to use hard drive to back the index of the event in the memory
a1.channels.c1.checkpointDir=/tmp/flume/checkpoint/consume_agent1/
# if you want to have two copie of the index backup, you can set below to true, the default value is false
a1.channels.c1.useDualCheckpoints=true

## define hdfs sink
a1.sinks.k1.type=hdfs
# with %Y-%m-%d, we can put event into appropriate time folder. The event timestamp is in the flume event header 
a1.sinks.k1.path=/raw_data/e_commerce/log/topic_log/%Y-%m-%d

# add a prefix to all generated log files
a1.sinks.k1.hdfs.filePrefix= log-

# disable timestamp round down. Because, we want group log by day. If you want to group log by every three hour, 
# you can set it to true, and add roundUnit=hour(can be minute, seconde), roundValue=3
a1.sinks.k1.hdfs.round=false

# Control the generated file type and codeC. In general, we compress the data to optimize the disk usage
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop


## link the component
# link source s1 to channel c1
a1.sources.s1.channels=c1
# link sink k1 to channel c1
a1.sinks.k1.channel=c1

```

If we save the above file as **kafka_flume_hdfs.conf**, we need to modify the flume_agent_management.sh to 
manage this agent

```shell
#!/bin/bash

base_dir="/opt/module/flume-1.9.0"
flume_agent_name="a1"
flume_agent_conf_file_path="/tmp/flume/conf"
flume_agent_conf_file_name="kafka_flume_hdfs.conf"
# Declare an array of string for flume nodes hostname in the cluster
declare -a hosts=("lin01" "lin02" "lin03")

# check the argument number
if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters, try flume_agent_management.sh start/stop"
    exit 1
fi

# for each argument, run the corresponding logic
case $1 in
"start")
for i in "${hosts[@]}"
do
  echo "-------------------- start flume agent process on $i -----------------------"
  ssh $i "${base_dir}/bin/flume-ng agent --conf-file ${flume_agent_conf_file_path}/${flume_agent_conf_file_name}
  --name ${flume_agent_name} -Dflume.root.logger=INFO,LOGFILE>${base_dir}/logs/${flume_agent_conf_file_name}_log.txt 2>&1 &"
done
;;

"stop")
for i in "${hosts[@]}"
do
  echo "-------------------- stop flume agent process on $i -----------------------"
  ssh $i "ps -ef | grep ${flume_agent_conf_file_name} | grep -v grep | awk '{print \$2}' | xargs -n1 kill -9"
done
;;

*)

echo "Illegal arguments, try kafka_cluster_management.sh start/stop"
exit 1
;;
esac
```

### 7.6.4 Small file management
When you launch the above agent, you will notice that it generate many small files, each patch is a file. We know 
that small files in hdfs has two major problems
- storage: Namenode needs to store metadata(150Byte) for all files. If the file has less than 150Byte, metadata use more
space than data itself.

- calculation: small data reduce IO, and each file is considered as a partition. Each partition is subject to a map task.
Create the context to run a task may be longer than the task because the data in the task is too small. Also, one map task
  reserves 1GB memory(MapReduce) by default, you may quickly saturate your cluster. And it's not optimal to use 1GB 
  memory to calculate 200Byte data.
  
To avoid the small file problem, we need to add the following line to the hdfs sink of the flume agent.

```text
# hdfs sink wait 10 seconds to create a new file to host new data
a1.sinks.k1.hdfs.rollInterval=10
# set rollsize to 128MB, if the current file size is bigger than 128MB, trigger the rolling to create a new file
a1.sinks.k1.hdfs.rollSize=134217728
# number of events to trigger a rolling, if set to 0, never trigger a rolling based on event number
a1.sinks.k1.hdfs.rollCount=0
```

## 7.7 Flume memory optimization

If your flume agent needs to handle many data, you may receive **OutOfMemory: GC overhead limit exceeded**. In this 
case, you need to modify the default flume memory configuration.

Go to the **/conf/flume-env.sh** add the following line. 
```shell
export JAVA_OPTS="-Xms4096m -Xmx4096m -Dcom.sun.management.jmxremote"
```
The default value is Xms100m, Xmx2000m.

- Xms: the minimum size of the jvm heap.

- Xmx: the maximum allow size of the jvm heap.

Some general flume memory config rules:
- We set memory to 4GB
- We set value of Xmx equals to Xms, this can avoid memory changes. If set different, it may cause frequent fullGC in jvm
