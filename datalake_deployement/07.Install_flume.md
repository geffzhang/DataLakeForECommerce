# 7. Flume

## 7.1 Install flume
You can download the latest stable version from this https://www.apache.org/dist/flume/stable/.
The current latest stable version is 1.9.0

Download the tar ball and untar it under **/opt/module/**

### 7.1.1 Configure env var for flume
Suppose the flume home path is /opt/module/flume-1.9.0

```shell

#add the flume home to the path
vim /etc/profile.d/flume.sh

# add following line to the file
export FLUME_HOME=/opt/module/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin

# check the flume home
source flume.sh 
echo $FLUME_HOME

```

### 7.1.2 Dependencies conflict with hadoop

On the same server, if you have installed hadoop 3.1.3 and flume 1.9.0. You will have a dependencies conflict. 
They both use the google core java lib Guava, but with different version. 

The easiest way is to delete the guava-11.0.2.jar under flume/lib/. Note, you must set env var for hadoop home. 
Otherwise, you will get class not found exception

## 7.2 Configure flume
All the important flume configuration files are located under **/path/to/flume/conf**. There are the four following 
files:

- flume-conf.properties: defines the flume agent
- flume-env.sh: defines the env var for running flume 
- flume-env.ps1.template
- log4j.properties

The most important config file for flume is **flume-env.sh**. Below is the minimum config which we need to run 
the flume.

```shell
vim flume-env.sh

#add the export java home
export JAVA_HOME=/opt/JAVA/jdk1.8.0_144

# Check the flume version
sh flume-ng version
```

## 7.3 Create a flume agent
A flume agent is an independent daemon process (JVM) in Flume. It receives the data (events) from clients or other 
agents and forwards it to its next destination (sink or agent). Flume may have more than one agent.

A flume agent contains three important components:
- source: receives data from the data generators and transfers it to one or more channels in the form of Flume events.
- channel: is a transient store which receives the events from the source and buffers them till they are consumed by 
  sinks. It acts as a bridge between the sources and the sinks.
- sink: stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels
  and delivers it to the destination. The destination of the sink might be another agent or the central stores.

### 7.3.1 Some popular sources
- Taildir: Watch the specified files, and tail them in nearly real-time once detected new lines appended to each file. 
  If the new lines are being written, this source will retry reading them in wait for the completion of the write.
  **This source is reliable and will not miss data** even when the tailing files rotate. It periodically writes the 
  last read position of each files on the given position file in JSON format. If Flume is stopped or down for some 
  reason, it can restart tailing from the position written on the existing position file.

- Avro source
- Thrift source 
  
- twitter 1% source 

### 7.3.1 Some popular channel
- File system channel: Reliable, high latency
- Memory channel: low latency, but reliable
- JDBC channel:
- kafka channel: low latency, reliable