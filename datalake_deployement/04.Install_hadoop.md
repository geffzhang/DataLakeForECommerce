# 4 Install a hadoop cluster

## 4.1 Network configuration
Note, the following instructions works on a centos 7
Suppose we have following vm in the cluster:
- Hadoop namenode: 192.168.1.2 ( dl01.pengfei.org )
- Hadoop datanode : 192.168.1.3 ( dl02.pengfei.org )
- Hadoop datanode : 192.168.1.4 ( dl03.pengfei.org )

Go to each vm, 
```shell
cd /etc/sysconfig/network-scripts
sudo vim ifcfg-ens33
```

Modify the config file for each vm to have appropriate static ip address.

```text
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
# don't use dhcp, use static config
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="d46d3db8-639d-41af-b412-550e266a756c"
DEVICE="ens33"
ONBOOT="yes"

# 
IPADDR=192.168.1.2
GATEWAY=192.168.1.1
NETMASK=255.255.255.0
DNS1=192.168.1.1
DNS2=8.8.8.8

```
## 4.2. Create user account

```shell
useradd hadoop
passwd hadoop
```
To allow ssh access to user hadoop, you also need to edit /etc/ssh/sshd_config

```shell
cd /etc/ssh/sshd_config

# add following line
AllowUsers hadoop
```


## 4.3 Add FQDN Mapping
Skip this step if your working environment has centralized hostname mapping. 
Edit /etc/hosts file on all master and slave servers and add following entries.

```shell
sudo vim /etc/hosts

192.168.1.2 dl01.pengfei.org
192.168.1.3 dl02.pengfei.org
192.168.1.4 dl03.pengfei.org

```

## 4.4 Configuring SSH key pair login
Hadoop framework itself doesn't need ssh, the administration tools like start.dfs.sh and stop.dfs.sh etc.. need it 
to start/stop various daemons. Thus, ssh must be installed and sshd must be running to use the Hadoop scripts 
that manage remote Hadoop daemons. So you need to create a ssh key paire for dl01 and copy the public 
key to dl02 and dl03. Repeat this for dl01 and dl02

```shell
su - hadoop
ssh-keygen -t rsa
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@dl01.pengfei.org
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@dl02.pengfei.org
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@dl03.pengfei.org
$ chmod 0600 ~/.ssh/authorized_keys
```

## 4.5 Install JDK

First check if your vm already have JDK or not. If yes, remove them

```shell
# rpm -qa returns all installed package
# grep -i means ignore case
# xargs -n1 returns the first value of the previous result one by one
# try echo 1 2 3 4 | xargs -n1 and echo 1 2 3 4 | xargs -n2
# rpm -e --nodeps remove package
sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
```

Download and Copy the jdk.tar.gz to /opt and untar it. 
Now we need to set up the env variables.

```shell
sudo vim /etc/profile.d/java.sh

# add following line
export JAVA_HOME=/opt/jdk1.8.0_121
export PATH=$PATH:/opt/jdk1.8.0_121/bin
```

Note we have many ways to set up env variables for shells. But we highly recommend you to use /etc/profil.d/. Becauee
it will load the env var for both login shell and non login shell. For more details please check my wiki 
pengfei.liu:admin_system:linux:bash_terminal_basics&s[]=login&s[]=shell 

![Env_var](https://raw.githubusercontent.com/pengfei99/DataLakeForECommerce/main/img/env_var.PNG)


