# 4 Install a hadoop cluster

## 4.1 Network configuration
Note, the following instructions works on a centos 7
Suppose we have following vm in the cluster:
- Hadoop namenode: 192.168.1.2 ( dl01.pengfei.org )
- Hadoop datanode : 192.168.1.3 ( dl02.pengfei.org )
- Hadoop datanode : 192.168.1.4 ( dl03.pengfei.org )

Go to each vm, 
```shell
cd /etc/sysconfig/network-scripts
sudo vim ifcfg-ens33
```

Modify the config file for each vm to have appropriate static ip address.

```text
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
# don't use dhcp, use static config
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="d46d3db8-639d-41af-b412-550e266a756c"
DEVICE="ens33"
ONBOOT="yes"

# 
IPADDR=192.168.1.2
GATEWAY=192.168.1.1
NETMASK=255.255.255.0
DNS1=192.168.1.1
DNS2=8.8.8.8

```
## 4.2. Create user account

```shell
useradd hadoop
passwd hadoop
```
To allow ssh access to user hadoop, you also need to edit /etc/ssh/sshd_config

```shell
cd /etc/ssh/sshd_config

# add following line
AllowUsers hadoop
```


## 4.3 Add FQDN Mapping
Skip this step if your working environment has centralized hostname mapping. 
Edit /etc/hosts file on all master and slave servers and add following entries.

```shell
sudo vim /etc/hosts

192.168.1.2 dl01.pengfei.org
192.168.1.3 dl02.pengfei.org
192.168.1.4 dl03.pengfei.org

```

## 4.4 Configuring SSH key pair login
Hadoop framework itself doesn't need ssh, the administration tools like start.dfs.sh and stop.dfs.sh etc.. need it 
to start/stop various daemons. Thus, ssh must be installed and sshd must be running to use the Hadoop scripts 
that manage remote Hadoop daemons. So you need to create a ssh key paire for dl01 and copy the public 
key to dl02 and dl03. Repeat this for dl01 and dl02

```shell
su - hadoop
ssh-keygen -t rsa
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@dl01.pengfei.org
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@dl02.pengfei.org
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@dl03.pengfei.org
$ chmod 0600 ~/.ssh/authorized_keys
```

## 4.5 Install JDK

First check if your vm already have JDK or not. If yes, remove them

```shell
# rpm -qa returns all installed package
# grep -i means ignore case
# xargs -n1 returns the first value of the previous result one by one
# try echo 1 2 3 4 | xargs -n1 and echo 1 2 3 4 | xargs -n2
# rpm -e --nodeps remove package
sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
```

Download and Copy the jdk.tar.gz to /opt and untar it. 
Now we need to set up the env variables.

```shell
sudo vim /etc/profile.d/java.sh

# add following line
export JAVA_HOME=/opt/jdk1.8.0_121
export PATH=$PATH:/opt/jdk1.8.0_121/bin
```

Note we have many ways to set up env variables for shells. But we highly recommend you to use /etc/profil.d/. Becauee
it will load the env var for both login shell and non login shell. For more details please check my wiki 
pengfei.liu:admin_system:linux:bash_terminal_basics&s[]=login&s[]=shell 

![Env_var](https://raw.githubusercontent.com/pengfei99/DataLakeForECommerce/main/img/env_var.PNG)

## 4.6 Configure hadoop as a Pseudo-Distributed cluster
Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.

### 4.6.1 Configure hdfs
Edit **etc/hadoop/core-site.xml**

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://pengfei.org:9000</value>
    </property>
</configuration>

```

Edit **etc/hadoop/hdfs-site.xml**

```xml
<configuration>
<property>
      <name>dfs.replication</name>
      <value>1</value>
   </property>

   <property>
      <name>dfs.name.dir</name>
      <value>file:////home/hadoop/hdfs/namenode</value>
   </property>

   <property>
      <name>dfs.data.dir</name>
      <value>file:///home/hadoop/hdfs/datanode</value>
   </property>
</configuration>

```

### 4.6.2 Configure yarn

You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running **ResourceManager daemon and NodeManager daemon**
in addition.

Edit **etc/hadoop/mapred-site.xml**

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

```

Edit **/hadoop/yarn-site.xml**

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```
## 4.9 Start the hadoop service

In a single node mode, you can use below command

```shell
$ cd /opt/hadoop/hadoop-3.2.2/sbin
$ ./start-all.sh
```
